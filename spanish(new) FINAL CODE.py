# -*- coding: utf-8 -*-
"""Spanish(NEW).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z4-ohtokN8-qgwWIzjwnjYvCcUWDeFM4
"""

## IMPORTANT DEPENDENCIES TO RUN FIRST

 !pip install git+https://github.com/openai/whisper.git
 !sudo apt update && sudo apt install ffmpeg

!pip install gTTS
from gtts import gTTS

!pip install SpeechRecognition

pip install googletrans==4.0.0-rc1

!pip install pydub

pip install whisper_timestamped

!pip install pydub moviepy
import moviepy.editor as mp
from pydub import AudioSegment
import whisper

def extract_audio_and_video(video_path, audio_output_path, video_output_path_without_audio):
    try:
        video = mp.VideoFileClip(video_path)

        # Save original audio
        audio = video.audio
        audio.write_audiofile(audio_output_path)

        # Save video without audio
        video_without_audio = video.set_audio(None)
        video_without_audio.write_videofile(video_output_path_without_audio, codec='libx264', audio_codec='aac')

        print("Extraction complete.")
    except Exception as e:
        print(f"Error: {e}")

def mp3_to_wav(mp3_path, wav_path):
    try:
        sound = AudioSegment.from_mp3(mp3_path)
        sound.export(wav_path, format="wav")
        print("Conversion complete.")
    except Exception as e:
        print(f"Error: {e}")

def transcribe_audio(audio_file, source_language):
    try:
        print("Transcribing audio track")
        model = whisper.load_model("medium")
        trans = model.transcribe(audio_file, language=source_language, verbose=False)
        # Remove timestamps
        trans_text = " ".join(word['word'] for word in trans)
        return trans_text
    except Exception as e:
        print(f"Error transcribing audio: {e}")
        return None

if __name__ == "__main__":
    video_path = input("Enter the path of the original video: ")
    audio_output_path_mp3 = "output_audio.mp3"
    video_output_path_without_audio = "video_without_audio.mp4"
    audio_output_path_wav = "output_audio.wav"
    source_language = input("Enter the language code of the audio (e.g., en-US for English-US): ")

    # Extract audio and video without audio
    extract_audio_and_video(video_path, audio_output_path_mp3, video_output_path_without_audio)

    # Convert mp3 to wav
    mp3_to_wav(audio_output_path_mp3, audio_output_path_wav)

    # Transcribe audio
    transcribed_text = transcribe_audio(audio_output_path_wav, source_language)
    if transcribed_text:
        print("Transcription:")
        print(transcribed_text)

import whisper_timestamped as whisper

audio_path = '/content/output_audio.mp3'  # Updated audio path
transcript_path = "transcript_without_timestamps.txt"

try:
    audio = whisper.load_audio(audio_path)
    model = whisper.load_model("small")

    result = whisper.transcribe(model, audio, language="en")

    # Write transcript to text file without timestamps
    with open(transcript_path, "w", encoding="utf-8") as txt_file:
        # Write text for each segment without timestamps
        for segment in result['segments']:
            text = segment['text'].strip()
            txt_file.write(text + "\n")
            print(text)

    print("Transcription saved to:", transcript_path)

except Exception as e:
    print(f"Error: {e}")

import os
import re
from moviepy.editor import VideoFileClip, AudioFileClip
from gtts import gTTS
from googletrans import Translator
from concurrent.futures import ThreadPoolExecutor
import time
import whisper_timestamped as whisper

def transcribe_audio(audio_path, transcript_path):
    try:
        audio = whisper.load_audio(audio_path)
        model = whisper.load_model("small")

        result = whisper.transcribe(model, audio, language="en")

        # Write transcript to text file without timestamps
        with open(transcript_path, "w", encoding="utf-8") as txt_file:
            for segment in result['segments']:
                text = segment['text'].strip()
                txt_file.write(text + "\n")
                print(text)

        print("Transcription saved to:", transcript_path)
        return transcript_path
    except Exception as e:
        print(f"Error: {e}")
        return None

def translate_text(text, target_language):
    translator = Translator()
    translation = translator.translate(text, dest=target_language)
    return translation.text

def translate_paragraph(paragraph, target_language):
    translated_paragraph = ""
    chunks = [paragraph[i:i+500] for i in range(0, len(paragraph), 500)]

    def translate_chunk(chunk):
        return translate_text(chunk, target_language)

    with ThreadPoolExecutor(max_workers=4) as executor:
        translated_chunks = executor.map(translate_chunk, chunks)

    for translated_chunk in translated_chunks:
        translated_paragraph += translated_chunk

    return translated_paragraph

def save_to_file(translated_text, file_path):
    try:
        with open(file_path, 'w', encoding='utf-8') as file:
            file.write(translated_text.strip())
        print("Translated text saved to:", file_path)
    except Exception as e:
        print(f"Error occurred while saving to file: {e}")

def text_to_speech(translated_text, output_audio_path, target_language):
    tts = gTTS(text=translated_text, lang=target_language)
    tts.save(output_audio_path)
    print("Text-to-speech conversion saved to:", output_audio_path)
    return output_audio_path

def merge_audio_with_video(input_video_path, audio_path, output_video_path):
    video_clip = VideoFileClip(input_video_path)
    audio_clip = AudioFileClip(audio_path)

    duration_diff = video_clip.duration - audio_clip.duration
    if duration_diff > 0:
        speed_factor = audio_clip.duration / video_clip.duration
        audio_clip = audio_clip.speedx(speed_factor)
    elif duration_diff < 0:
        speed_factor = video_clip.duration / audio_clip.duration
        video_clip = video_clip.speedx(speed_factor)

    video_clip = video_clip.set_audio(audio_clip)
    video_clip.write_videofile(output_video_path, codec='libx264', audio_codec='aac')

    video_clip.close()
    audio_clip.close()

def main():
    audio_path = '/content/output_audio.mp3'  # Updated audio path
    transcript_path = "transcript_without_timestamps.txt"
    translated_text_path = "/content/india_translated.txt"
    target_language = "hi"  # Target language code, e.g., "fr" for French
    input_video_path = "/content/video_without_audio.mp4"
    output_video_path = "/content/output_video_with_audio.mp4"
    output_audio_path = "temp_out.mp3"

    start_time = time.time()

    # Step 1: Transcribe audio
    transcript = transcribe_audio(audio_path, transcript_path)

    if transcript:
        with open(transcript, 'r', encoding='utf-8') as file:
            paragraph = file.read()
        print("Original Text:")
        print(paragraph)

        # Step 2: Translate text
        translated_paragraph = translate_paragraph(paragraph, target_language)
        if translated_paragraph:
            print("\nTranslated Text:")
            print(translated_paragraph)

            # Step 3: Save translated text
            save_to_file(translated_paragraph, translated_text_path)

            # Step 4: Text-to-Speech conversion
            tts_audio = text_to_speech(translated_paragraph, output_audio_path, target_language)

            # Step 5: Merge audio with video
            if os.path.exists(input_video_path) and os.path.exists(tts_audio):
                merge_audio_with_video(input_video_path, tts_audio, output_video_path)
                print("Video with merged audio saved successfully.")
            else:
                print("Input video or audio file not found.")
        else:
            print("Translation failed.")
    else:
        print("Text extraction failed.")

    end_time = time.time()
    print("Execution time:", end_time - start_time, "seconds")

if __name__ == "__main__":
    main()

def wer(reference, hypothesis):

    ref_words = reference.split()
    hyp_words = hypothesis.split()

    # Initialize dynamic programming table
    dp = [[0] * (len(hyp_words) + 1) for _ in range(len(ref_words) + 1)]

    # Fill the dynamic programming table
    for i in range(len(ref_words) + 1):
        for j in range(len(hyp_words) + 1):
            if i == 0:
                dp[i][j] = j
            elif j == 0:
                dp[i][j] = i
            else:
                dp[i][j] = min(dp[i-1][j-1] + (0 if ref_words[i-1] == hyp_words[j-1] else 1),
                               dp[i-1][j] + 1,
                               dp[i][j-1] + 1)

    # Return the WER (normalized edit distance)
    return float(dp[len(ref_words)][len(hyp_words)]) / len(ref_words)

# Read the English transcript (reference) from file
with open("/content/transcript_without_timestamps.txt", "r", encoding="utf-8") as f:
    reference_transcript = f.read().strip()

# Read the translated transcript (hypothesis) from file
with open("/content/india_translated.txt", "r", encoding="utf-8") as f:
    translated_transcript = f.read().strip()

# Calculate the Word Error Rate (WER)
wer_score = wer(reference_transcript, translated_transcript)
print("Word Error Rate:", wer_score)

# Install required packages
!pip install googletrans==4.0.0-rc1 bert-score sentence-transformers nltk rouge-score

import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Import libraries
from googletrans import Translator
from bert_score import score
from sentence_transformers import SentenceTransformer, util
from nltk.translate.bleu_score import sentence_bleu
from nltk.translate.meteor_score import meteor_score
from rouge_score import rouge_scorer

# Initialize the translator and models
translator = Translator()
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Function to compute BERTScore
def compute_bertscore(reference, candidate):
    P, R, F1 = score([candidate], [reference], lang='en')
    return F1.item()

# Function to compute sBERT similarity
def compute_sbert(reference, candidate):
    embeddings = model.encode([reference, candidate])
    similarity = util.pytorch_cos_sim(embeddings[0], embeddings[1])
    return similarity.item()

# Function to compute BLEU score
def compute_bleu(reference, candidate):
    reference = [nltk.word_tokenize(reference)]
    candidate = nltk.word_tokenize(candidate)
    score = sentence_bleu(reference, candidate)
    return score

# Function to compute METEOR score
def compute_meteor(reference, candidate):
    reference = nltk.word_tokenize(reference)
    candidate = nltk.word_tokenize(candidate)
    return meteor_score([reference], candidate)

# Function to compute ROUGE scores
def compute_rouge(reference, candidate):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
    scores = scorer.score(reference, candidate)
    return scores

# Aggregate evaluation scores
def evaluate_translation(reference, candidate):
    bertscore_f1 = compute_bertscore(reference, candidate)
    sbert_similarity = compute_sbert(reference, candidate)
    bleu_score = compute_bleu(reference, candidate)
    meteor = compute_meteor(reference, candidate)
    rouge_scores = compute_rouge(reference, candidate)

    return {
        "BERTScore_F1": bertscore_f1,
        "sBERT_Similarity": sbert_similarity,
        "BLEU_Score": bleu_score,
        "METEOR_Score": meteor,
        "ROUGE_Scores": rouge_scores
    }

# Reference and translated sentences
reference_sentence = "But while we are fighting every day to build up our nation"
translated_sentence = "Mais pendant que nous nous battons tous les jours pour construire notre nation"

# Evaluate the translation
evaluation_scores = evaluate_translation(reference_sentence, translated_sentence)

# Display evaluation scores with descriptive messages
print(f"BERTScore F1 should be high. Obtained score: {evaluation_scores['BERTScore_F1']}")
print(f"sBERT Similarity should be high. Obtained score: {evaluation_scores['sBERT_Similarity']}")
print(f"BLEU Score should be high. Obtained score: {evaluation_scores['BLEU_Score']}")
print(f"METEOR Score should be high. Obtained score: {evaluation_scores['METEOR_Score']}")
print(f"ROUGE-1 Score should be high. Obtained scores: {evaluation_scores['ROUGE_Scores']['rouge1']}")
print(f"ROUGE-L Score should be high. Obtained scores: {evaluation_scores['ROUGE_Scores']['rougeL']}")